@misc{rudin2021walk,
  doi = {10.48550/ARXIV.2109.11978},
  
  url = {https://arxiv.org/abs/2109.11978},
  
  author = {Rudin, Nikita and Hoeller, David and Reist, Philipp and Hutter, Marco},
  
  keywords = {Robotics (cs.RO), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Peng_2021,
	doi = {10.1145/3450626.3459670},
  
	url = {https://doi.org/10.1145%2F3450626.3459670},
  
	year = 2021,
	month = {jul},
  
	publisher = {Association for Computing Machinery ({ACM})},
  
	volume = {40},
  
	number = {4},
  
	pages = {1--20},
  
	author = {Xue Bin Peng and Ze Ma and Pieter Abbeel and Sergey Levine and Angjoo Kanazawa},
  
	title = {{AMP}
},
  
	journal = {{ACM} Transactions on Graphics}
}

@article{sharma2019dynamics,
  title={Dynamics-aware unsupervised discovery of skills},
  author={Sharma, Archit and Gu, Shixiang and Levine, Sergey and Kumar, Vikash and Hausman, Karol},
  journal={arXiv preprint arXiv:1907.01657},
  year={2019}
}

@article{sutton1999options,
title = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
journal = {Artificial Intelligence},
volume = {112},
number = {1},
pages = {181-211},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00052-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000521},
author = {Richard S. Sutton and Doina Precup and Satinder Singh},
keywords = {Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes},
abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include optionsâ€”closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.}
}