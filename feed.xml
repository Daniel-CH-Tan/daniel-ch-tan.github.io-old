<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://daniel-ch-tan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://daniel-ch-tan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-07-23T19:02:46+00:00</updated><id>https://daniel-ch-tan.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Transformers are Linear-Bounded Automata</title><link href="https://daniel-ch-tan.github.io/blog/2022/nlu-complexity/" rel="alternate" type="text/html" title="Transformers are Linear-Bounded Automata"/><published>2022-05-22T00:00:00+00:00</published><updated>2022-05-22T00:00:00+00:00</updated><id>https://daniel-ch-tan.github.io/blog/2022/nlu-complexity</id><content type="html" xml:base="https://daniel-ch-tan.github.io/blog/2022/nlu-complexity/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recently, I’ve been thinking about natural language understanding and its relationship to complexity theory.</p> <p>First some definitions. A <em>language</em> is a set of strings over an alphabet. A <em>grammar</em> can be abstractly thought of as an algorithm that parses a string and <em>decides</em> whether it is in the language or not. A grammar always induces a corresponding language</p> <p>The <a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy">Chomsky hierarchy</a> provides a classification of languages. The simplest are regular expressions and context-free grammars, which are induced by finite-state automata and pushdown automata respectively.</p> <p>I’m more interested in the two more complex ones, namely <em>context-sensitive grammers</em> and <em>recursively enumerable</em> languages. The latter is induced by a Turing Machine (TM) and the former by a Linear-Bounded Automaton (LBA). An LBA is simply a TM which is constrained to use memory linear in its input, as opposed to a TM which can use infinite memory.</p> <h2 id="transformers-as-lbas">Transformers as LBAs</h2> <p>I think this is interesting because transformers are LBAs. When processing a sequence, a transformer performs computations on a sequence of input token embeddings (linear in input size) and its own weights (constant in input size). Hence transformers are LBAs.</p> <p>Contrast this to RNNs, which have a <em>constant</em> memory size (the value of the hidden state) to process sequence inputs.</p> <h2 id="natural-language-as-a-context-free-grammar">Natural Language as a Context-Free Grammar</h2> <p>Recall that CFGs are exactly the class of languages decided by LBAs, and transformers are LBAs. Therefore, the empirical observation that transformers do so well on natural language tasks may be an indication that natural language can be modelled very well as a CFG.</p> <p>Some work on understanding language as a CFG: <a href="https://github.com/delph-in/docs/wiki/">DELPH-IN</a></p>]]></content><author><name>Daniel C.H Tan</name></author><summary type="html"><![CDATA[Interesting implications of considering natural language from the perspective of computational complexity theory]]></summary></entry><entry><title type="html">Skill Learning</title><link href="https://daniel-ch-tan.github.io/blog/2022/skill-learning/" rel="alternate" type="text/html" title="Skill Learning"/><published>2022-01-13T00:00:00+00:00</published><updated>2022-01-13T00:00:00+00:00</updated><id>https://daniel-ch-tan.github.io/blog/2022/skill-learning</id><content type="html" xml:base="https://daniel-ch-tan.github.io/blog/2022/skill-learning/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>As humans, we have learned certain patterns of whole-body actuation as high-level “skills” (walking, running, jumping, etc.) In planning our bodily actions, we do not consciously control each and every actuator (muscle) in our body. Rather,we largely constrain ourselves to these general and re-usable patterns, adapting and composing them on the fly to fit the situation. Why not do the same for robots?</p> <p>In this blog post, I will describe the motivations and approaches for skill learning in control problems, i.e. to <strong>learn</strong> and <strong>discover</strong> a <strong>diverse</strong> and <strong>high-quality</strong> set of <strong>action primitives</strong> that <strong>transfer well</strong> to downstream tasks.</p> <h2 id="motivation">Motivation</h2> <h3 id="the-cereal-making-robot">The Cereal-Making Robot</h3> <p>An average ten-year-old could probably walk into the kitchen and make themselves a bowl of cereal unsupervised. Yet the same capability remains out of reach of modern robots. (Or at the least, it has yet to be demonstrated!) Why is that? (Example credit: Chelsea Finn, <a href="https://thegradientpub.substack.com/p/chelsea-finn-on-meta-learning-and#details">podcast episode</a> hosted by <a href="https://thegradientpub.substack.com/s/podcast">The Gradient</a>).</p> <div class="row mt-3" style="position: relative"> <div class="col-sm mt-3 mt-md-0" style="width: 50%"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/skill_learning/cereal-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/skill_learning/cereal-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/skill_learning/cereal-1400.webp"/> <img src="/assets/img/skill_learning/cereal.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Making cereal is so easy, even kids can do it! </div> <p>More generally, we would like to develop control frameworks that enable robots to accomplish <strong>complex tasks</strong> in the <strong>real world</strong>. </p> <h3 id="skills-as-building-blocks">Skills as Building Blocks</h3> <p>One method of enabling complex task-oriented behaviour, is to adopt an incremental approach and try to learn to solve simpler subproblems first, then compose those solutions into a full solution. As programmers, this sounds inherently appealing. Going back to the cereal analogy, we might come up with a list of lower-level skills that need to be learned first, such as <strong>finding</strong> the necessary ingredients, <strong>picking</strong> up the cereal box, <strong>pouring</strong> the milk into a bowl, and <strong>carrying</strong> it out of the kitchen while maintaining good <strong>balance</strong>.</p> <p>If we had access to high-quality action primitives that were able to solve each of these subproblems, the overall problem reduces to a simple task of finding the right skills and sequencing them in the right order. Conversely, needing to learn to solve all of these subproblems from scratch makes the overall problem exponentially more difficult.</p> <p>The core problem of skill learning is therefore to discover and learn a diverse set of high-quality action primitives that transfer well to downstream tasks.</p> <h3 id="skills-for-dexterity">Skills for Dexterity</h3> <p>It has been said that dexterity is the “last open frontier in robotics”.</p> <p>One reason for this is that learning-based approaches struggle is the difficulty of exploration in high-dimensional control space. For a humanoid morphology, even maintaining balance is already a rather challenging task, let alone learning to move in an agile, safe, and efficient way or exercise fine motor skills in handling everyday objects.</p> <h2 id="related-work">Related work</h2> <h3 id="representation-learning">Representation Learning</h3> <p>Skill learning can be thought of as representation learning. However, while most representation learning methods aim to construct informative representations of the observations space, we are instead focusing on the action space. The space of all valid action primitives scales as \(\dim(A^T)\); yet the space of useful action primitives could be hypothesised to be restricted to a low-dimensional manifold within this combinatorially large space. Reducing the dimension of the action space has obvious benefits for improving interpretability as well as the efficiency of exploration.</p> <h3 id="options-theory">Options Theory</h3> <p>One useful mathematical formulation for skill learning is the options framework introduced in <d-cite key="sutton1999options"></d-cite>. For a given Markov Decision Process (MDP), options are auxilliary policies that conditionally execute upon entering certain states and run until their termination condition is met.</p> <h3 id="latent-variable-policies">Latent-Variable Policies</h3> <p>In practice, it has been popular <d-cite key="sharma2019dynamics"></d-cite> <d-cite key="Peng_2021"></d-cite> to implement skill learning using a latent-conditioned policy. Each instantiation of the latent variable corresponds to a single option, and the choice of distribution over the latents induces a family of options. Assuming sufficient regularity of the latent space, the latent-variable representations can be interpolated to sample novel skills.</p> <h3 id="planning-with-subgoals">Planning with subgoals</h3> <p>A similar line of reasoning is employed in <strong>subgoal</strong>-conditioned learning and planning; however, while subgoals are akin to static milestones that need to be achieved in order to reach the main goal, skills are akin to the known routes that take us safely and quickly between subgoals.</p> <h2 id="learning-skills">Learning Skills</h2> <p>Learning a skill refers to learning how to execute the skill, given that we already know what it is. Depending on choice of representation, this may be very easy; for example, if we have a skill represented as a state-action trajectory, then simple behaviour cloning would enable the skill to be learned. For the task of simulated locomotion, ASE <d-cite key="peng2022ase"></d-cite> and its predecessors used motion-capture data from known quadrupedal and bipedal experts (i.e. animals and humans).</p> <p>For a given control task and action space, domain knowledge may also give us clues about how to design families of high-quality skills. For example, in legged locomotion, classical control methods have done well by using gait-parameterized actuation as a family of skills. Similarly, generalized hybrid-zero dynamics has been an effective heuristic for designing skills for bipedal locomotion.</p> <p>However, in practice, such data and domain knowledge may not be readily available, which leads us to…</p> <h2 id="discovering-skills">Discovering Skills</h2> <p>Instead of relying on prior knowledge, we can aim to discover skills simply from pure exploration. However, in the absence of extrinsic inductive biases, skill discovery requires intrinsic priors on high-quality skills, e.g diversity and consistency of the skill set <d-cite key="sharma2019dynamics"></d-cite>. By optimizing these intrinsic metrics, we hope that skills emerge autonomously from the learning process.</p> <p>Compared to the previous approaches, there is no guarantee of quality of individual skills in the learned set. However, by maximizing diversity, we can hope that the learned skill set contains sufficient high-quality skills that transfer well to downstream tasks.</p> <h2 id="transferring-skills-to-tasks">Transferring Skills to Tasks</h2> <p>Okay, so suppose that we have a collection of high-quality skills. How do we use them in a task-oriented way?</p> <h3 id="hierarchical-learning">Hierarchical Learning</h3> <p>A suitable collection of skills can be directly used as a low-level controller. A high-level planner can then use the skill space as its action space, selecting and sequencing options to solve specific tasks.</p> <p>Doing this has several benefits. First and foremost, the high-level policy does not need to learn skills, but can simply make use of them in a zero-shot manner. Furthermore, exploration in a highly informative, low-dimensional latent space is likely to be much easier than exploration in a high-dimensional, noisy space, greatly improving sample efficiency on downstream tasks.</p> <p>To understand this, consider that simple exploration strategies in the skill space can translate to highly structured and efficient exploration in the action space. </p> <h3 id="teacher-student-learning">Teacher-Student Learning</h3> <p>Another perspective is to simply treat the learned skills as ‘teachers’ that generate off-policy state-action-reward histories, which are made available to student policies. This could be used by off-policy RL algorithms or even by imitation learning methods.</p> <p>Compared to the hierarchical approach, this has the benefit of fewer architectural constraints on the final policy. In turn, it likely trades off some of the gains in sample efficiency from directly re-using learned skills as action primitives.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, I discussed the motivating factors and benefits/disadvantages of the skills framework vis-a-vis reinforcement learning. I also reviewed some of the current literature in this field.</p> <p>In the next blog post, I’ll talk more about my current progress in applying a skills-based framework to robotic control tasks.</p>]]></content><author><name>Daniel C.H Tan</name></author><summary type="html"><![CDATA[Thoughts on learning general and re-usable skills for continuous-control tasks in robotics]]></summary></entry></feed>